{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d3d93364f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_set = datasets.MNIST('./data/', train=True, download=True, transform = transform)\n",
    "test_set = datasets.MNIST('./data/', train=False, download=True, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_set_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Parameters: Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "print('CNN Parameters:',model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_set_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "images, labels = next(iter(train_set_loader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logps = model(images) #log probabilities\n",
    "loss = criterion(logps, labels) #calculate the NLL loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward pass: \n",
      " None\n",
      "After backward pass: \n",
      " tensor([[-2.8627e-03, -2.8627e-03, -2.8627e-03,  ..., -2.8627e-03,\n",
      "         -2.8627e-03, -2.8627e-03],\n",
      "        [ 2.3988e-03,  2.3988e-03,  2.3988e-03,  ...,  2.3988e-03,\n",
      "          2.3988e-03,  2.3988e-03],\n",
      "        [-2.7834e-04, -2.7834e-04, -2.7834e-04,  ..., -2.7834e-04,\n",
      "         -2.7834e-04, -2.7834e-04],\n",
      "        ...,\n",
      "        [-9.4517e-05, -9.4517e-05, -9.4517e-05,  ..., -9.4516e-05,\n",
      "         -9.4516e-05, -9.4516e-05],\n",
      "        [ 4.8693e-04,  4.8693e-04,  4.8693e-04,  ...,  4.8693e-04,\n",
      "          4.8693e-04,  4.8693e-04],\n",
      "        [-3.3938e-03, -3.3938e-03, -3.3938e-03,  ..., -3.3938e-03,\n",
      "         -3.3938e-03, -3.3938e-03]])\n"
     ]
    }
   ],
   "source": [
    "print('Before backward pass: \\n', model[0].weight.grad)\n",
    "loss.backward()\n",
    "print('After backward pass: \\n', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.6405591621899656\n",
      "Epoch 1 - Training loss: 0.2878766824314589\n",
      "Epoch 2 - Training loss: 0.22646720947296634\n",
      "Epoch 3 - Training loss: 0.1847485890473003\n",
      "Epoch 4 - Training loss: 0.15539510635488324\n",
      "Epoch 5 - Training loss: 0.13076371260122385\n",
      "Epoch 6 - Training loss: 0.11432365281705949\n",
      "Epoch 7 - Training loss: 0.10212104376762915\n",
      "Epoch 8 - Training loss: 0.09103792511335791\n",
      "Epoch 9 - Training loss: 0.08290458811240504\n",
      "Epoch 10 - Training loss: 0.07456968353514168\n",
      "Epoch 11 - Training loss: 0.06811215195095917\n",
      "Epoch 12 - Training loss: 0.061133339534452096\n",
      "Epoch 13 - Training loss: 0.057610093862928355\n",
      "Epoch 14 - Training loss: 0.05255973045411371\n",
      "\n",
      "Training Time (in minutes) = 7.4702536503473915\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
    "time0 = time()\n",
    "epochs = 15\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in train_set_loader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        #This is where the model learns by backpropagating\n",
    "        loss.backward()\n",
    "        \n",
    "        #And optimizes its weights here\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(train_set_loader)))\n",
    "print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Images Tested = 10000\n",
      "9729\n",
      "\n",
      "Model Accuracy = 0.9729\n"
     ]
    }
   ],
   "source": [
    "correct_count, all_count = 0, 0\n",
    "predicted_labels = []\n",
    "actual_labels = []\n",
    "for images,labels in test_set_loader:\n",
    "    for i in range(len(labels)):\n",
    "        img = images[i].view(1, 784)\n",
    "        with torch.no_grad():\n",
    "            logps = model(img)\n",
    "\n",
    "\n",
    "        ps = torch.exp(logps)\n",
    "        probab = list(ps.numpy()[0])\n",
    "        pred_label = probab.index(max(probab))\n",
    "        true_label = labels.numpy()[i]\n",
    "        \n",
    "        predicted_labels.append(pred_label)\n",
    "        actual_labels.append(true_label)\n",
    "        \n",
    "        if(true_label == pred_label):\n",
    "            correct_count += 1\n",
    "        all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(correct_count)\n",
    "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 4, 1, 4, 2, 6, 6, 9, 6]\n",
      "[1, 3, 4, 1, 4, 2, 6, 6, 9, 6]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# confusion_matrices(clf_nb, stop_words_nb, ngram_nb, 'Confusion Matrix - Naive Bayes',x_train, y_train, x_test, y_test)\n",
    "print(predicted_labels[:10])\n",
    "print(actual_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "           \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "#     print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (10, 10))\n",
    "    im = ax.imshow(cm,cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    \n",
    "    x_classes = classes \n",
    "    classes.reverse()\n",
    "    \n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),       \n",
    "           xticklabels=x_classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), ha=\"right\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    \n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            color=\"white\" if cm[i, j] > 500 else \"black\"\n",
    "            ax.text(j, i, format(cm[i, j]), ha=\"center\", va=\"center\", color=color)\n",
    "    \n",
    "    ax.set_xticks(np.arange(cm.shape[1]+1)-.5)\n",
    "    ax.set_yticks(np.arange(cm.shape[0]+1)-.5)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of times to rerun the training dataset\n",
    "epochs = 1\n",
    "\n",
    "# How many images to feed through the network each time\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "\n",
    "# Step size for gradiant decent\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Gives the network a chance to jump out of a local minima\n",
    "momentum = 0.5\n",
    "\n",
    "# Use a constant seed for randomness so that reruns becomes predictable\n",
    "random_seed = 1\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(28 * 28, 50),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(50, 10),\n",
    "        nn.LogSoftmax(1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_set):\n",
    "    total = 0\n",
    "    nr_correct = 0\n",
    "    \n",
    "    for images, labels in test_set:\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        result = model(images)\n",
    "        \n",
    "        _, predictions = torch.max(result, 1)\n",
    "        total += images.size(0)\n",
    "        nr_correct += torch.sum(predictions == labels).item()\n",
    "                \n",
    "    return nr_correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1/1, Train Loss: 0.58956212, Training Accuracy: 0.81250000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9075"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model()\n",
    "accuracy = train_model(model)\n",
    "test_model(model, test_set_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-06369324405c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mactual_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m784\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "correct_count, all_count = 0, 0\n",
    "predicted_labels = []\n",
    "actual_labels = []\n",
    "for images,labels in test_set:\n",
    "    for i in range(len(labels)):\n",
    "        img = images[i].view(1, 784)\n",
    "        with torch.no_grad():\n",
    "            logps = model(img)\n",
    "\n",
    "        ps = torch.exp(logps)\n",
    "        probab = list(ps.numpy()[0])\n",
    "        pred_label = probab.index(max(probab))\n",
    "        true_label = labels.numpy()[i]\n",
    "        \n",
    "        predicted_labels.append(pred_label)\n",
    "        actual_labels.append(true_label)\n",
    "        \n",
    "        if(true_label == pred_label):\n",
    "            correct_count += 1\n",
    "        all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(correct_count)\n",
    "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9,]\n",
    "\n",
    "plot_confusion_matrix(predicted_labels, actual_labels, label_list, title='Baseline Model Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
