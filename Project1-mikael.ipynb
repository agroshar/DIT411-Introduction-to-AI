{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the libraries which are used for importing the dataset and creating the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import random\n",
    "import itertools\n",
    "import time\n",
    "import heapq\n",
    "import gc\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create static variables that will be used later. These variables consists of values such as the rate at which the network learns from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of times to rerun the training dataset\n",
    "epochs = 1\n",
    "\n",
    "# How many images to feed through the network each time\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "\n",
    "# Step size for gradiant decent\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Gives the network a chance to jump out of a local minima\n",
    "momentum = 0.5\n",
    "\n",
    "# Use a constant seed for randomness so that reruns becomes predictable\n",
    "random_seed = 1\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\" \n",
    "else:  \n",
    "    dev = \"cpu\"  \n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code blocks provides the data set with handwritten digits. It converts the images into Pytorch tensors and normalizes the pixels into the range 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = torch.utils.data.DataLoader(torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                             torchvision.transforms.Normalize((0.5,), (0.5,))])),batch_size=batch_size_train, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "test_set = torch.utils.data.DataLoader(torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize((0.5,), (0.5,))])),batch_size=batch_size_test, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following line of code simply outputs the structure of the data. The output here shows that each batch in the training data contains 64 images and that each image has 28x28 pixels dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = enumerate(training_set)\n",
    "_, (example_data, example_targets) = next(examples)\n",
    "example_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block below plots different numbers from the dataset to show numbers and different ways people wrote them down. Each image has a ground truth which simply means what number does the image represent. This is a way to show that everyone has a different handwriting and the network should be capable of recognizing most of the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Label: {}\".format(example_targets[i]))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function simply creates and returns a model that contains 3 layers: a 28x28 input layer, a hidden layer of 50 nodes and an output layer of size 10. The LogSoftmax is for normalizing the output vector to a range between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1296\n"
     ]
    }
   ],
   "source": [
    "def hyper_parameter_permutations():\n",
    "    layer_sizes = [32, 64, 128]\n",
    "    hidden_layers = [list(n) for n in itertools.permutations(layer_sizes, 1)]\\\n",
    "                  + [[a, b] for (a, b) in itertools.permutations(layer_sizes, 2) if a >= b]\n",
    "    \n",
    "    channel_sizes = [4, 8, 16]\n",
    "    conv_layers = [list(n) for n in itertools.permutations(channel_sizes, 1)]\\\n",
    "                + [[a, b] for (a, b) in itertools.permutations(channel_sizes, 2) if a <= b]\n",
    "    \n",
    "    kernel_sizes = [5]\n",
    "    activation_functions = ['ReLU', 'Sigmoid', 'Swish']\n",
    "    criterions = ['CrossEntropy']\n",
    "    \n",
    "    #Adding momentum directly to the optimizer since it's only used for SGD\n",
    "    optimizers = [('SGD', 0.1), ('SGD', 0.5), ('SGD', 1.0), 'Adam']\n",
    "    learning_rates = [0.1, 0.01, 0.001]\n",
    "\n",
    "    return itertools.product(\n",
    "        hidden_layers,\n",
    "        conv_layers,\n",
    "        kernel_sizes,\n",
    "        activation_functions,\n",
    "        criterions,\n",
    "        optimizers,\n",
    "        learning_rates\n",
    "    )\n",
    "\n",
    "\n",
    "def permutation_to_dictionary(hidden_layers, channel_sizes, kernel_size, act_fun, criterion, optimizer, learning_rate):\n",
    "    conv_layers = []\n",
    "    in_channels = 1\n",
    "    side_length = 28\n",
    "    for channel_size in channel_sizes:\n",
    "        out_channels = channel_size\n",
    "        conv_layers.append(\n",
    "            {\n",
    "                'in_channels': in_channels,\n",
    "                'out_channels': out_channels,\n",
    "                'kernel_size': kernel_size\n",
    "            }\n",
    "        )\n",
    "        in_channels = out_channels\n",
    "    \n",
    "        side_length -= kernel_size - 1\n",
    "        side_length //= 2\n",
    "        \n",
    "    if type(optimizer) is tuple:\n",
    "        momentum = optimizer[1]\n",
    "        optimizer = optimizer[0]\n",
    "    else:\n",
    "        momentum = 0\n",
    "    \n",
    "    regular_layers = [side_length * side_length * out_channels] + hidden_layers + [10]\n",
    "    return {\n",
    "        'conv_layers': conv_layers,\n",
    "        'regular_layers': regular_layers,\n",
    "        'activation_function': act_fun,\n",
    "        'optimizer': optimizer,\n",
    "        'criterion': criterion,\n",
    "        'learning_rate': learning_rate,\n",
    "        'momentum': momentum\n",
    "    }\n",
    "    \n",
    "    \n",
    "i = 0\n",
    "for n in hyper_parameter_permutations():\n",
    "    #print(n)\n",
    "    i += 1\n",
    "print(i)\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, hyper_params):\n",
    "        super(Model, self).__init__()\n",
    "        self.hyper_params = hyper_params\n",
    "        \n",
    "        layers = []\n",
    "        for n in self.hyper_params['conv_layers']:\n",
    "            conv_layer = nn.Conv2d(\n",
    "                n['in_channels'], \n",
    "                n['out_channels'], \n",
    "                n['kernel_size']\n",
    "            )\n",
    "            layers.append(conv_layer)\n",
    "            layers.append(self.get_activation_function())\n",
    "            layers.append(nn.MaxPool2d(2))\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        \n",
    "\n",
    "        layers = []\n",
    "        for i in range(len(self.hyper_params['regular_layers']) - 1):\n",
    "            nr_in = self.hyper_params['regular_layers'][i];\n",
    "            nr_out = self.hyper_params['regular_layers'][i + 1];\n",
    "            layers.append(nn.Linear(nr_in, nr_out))\n",
    "            layers.append(self.get_activation_function())\n",
    "        \n",
    "        # The output layer should not have an activation function\n",
    "        layers.pop()\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "        \n",
    "        learning_rate = self.hyper_params['learning_rate']\n",
    "        momentum = self.hyper_params['momentum']\n",
    "        optimizer = self.hyper_params['optimizer']\n",
    "        if optimizer == 'SGD':\n",
    "            self.optimizer = optim.SGD(self.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        elif optimizer == 'Adam':\n",
    "            self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        else:\n",
    "            raise Exception('Invalid optimizer')\n",
    "            \n",
    "        criterion = self.hyper_params['criterion']\n",
    "        if criterion == 'CrossEntropy':\n",
    "            self.criterion = nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            raise Exception('Invalid criterion')\n",
    "        \n",
    "        \n",
    "    def get_activation_function(self):\n",
    "        fun = self.hyper_params['activation_function']\n",
    "        if fun == 'ReLU':\n",
    "            return nn.ReLU()\n",
    "        if fun == 'Sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        if fun == 'Swish':\n",
    "            return Swish()\n",
    "        \n",
    "        \n",
    "    def forward(self, tensor):\n",
    "        tensor = self.conv(tensor)\n",
    "        tensor = tensor.reshape(tensor.size(0), -1)\n",
    "        return self.net(tensor)\n",
    "    \n",
    "    \n",
    "    def fit(self, tensor, labels):\n",
    "        tensor = tensor.to(device)\n",
    "        labels = labels.to(device)\n",
    "        result = self(tensor)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.criterion(result, labels)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        _, predictions = torch.max(result, 1)\n",
    "        total = tensor.size(0)\n",
    "        nr_correct = torch.sum(predictions == labels).item()\n",
    "        \n",
    "        return nr_correct / total\n",
    "    \n",
    "    \n",
    "    #Needed to not compare models stored in a tuple\n",
    "    def __lt__(self, m):\n",
    "        return False\n",
    "    \n",
    "    \n",
    "class Swish(nn.Module):\n",
    "    def forward(self, input_tensor):\n",
    "        return input_tensor * torch.sigmoid(input_tensor)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block trains the network with the handwritten digit data (training_set). It iterates over an n amount of epochs. It also iterates over each batch from the training_set. At each iteration, it adjusts the weights as well as calculating the loss. This allows us to see how accurate is the prediction of each batch. The accuracy is stored in an array which helps to plot the training process of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "    total = 0\n",
    "    nr_correct = 0\n",
    "\n",
    "    accuracies = []\n",
    "    for n in range(0, epochs):\n",
    "        for (batch_index, (images, labels)) in enumerate(training_set):\n",
    "            accuracy = model.fit(images, labels)\n",
    "            accuracies.append(accuracy)\n",
    "            \n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this function create a test model which can be used to compare to the training model. It is used to check if the model generalized to the other data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_set):\n",
    "    total = 0\n",
    "    nr_correct = 0\n",
    "    \n",
    "    for images, labels in test_set:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        result = model(images)\n",
    "        \n",
    "        _, predictions = torch.max(result, 1)\n",
    "        total = images.size(0)\n",
    "        nr_correct = torch.sum(predictions == labels).item()\n",
    "                \n",
    "    return nr_correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function within the codeblock below uses the information obtained from the training to plot the rate at which the network learns about the data. The plot contains the amount of training batches and the accuracy of the model during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_rate(batch_accuracies):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(\n",
    "        range(0, len(batch_accuracies)), \n",
    "        batch_accuracies\n",
    "    )\n",
    "\n",
    "    ax.set_ylim((0, 1))\n",
    "    ax.set(\n",
    "        xlabel='training batches', \n",
    "        ylabel='accuracy',\n",
    "        title='Model accuracy during training'\n",
    "    )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code simply calls out the previously mentioned functions in order to create the model, train it and plot it the information about its accuracy during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search():\n",
    "    gc.collect()\n",
    "    before = time.time()\n",
    "\n",
    "    models = []\n",
    "    \n",
    "    permutations = list(hyper_parameter_permutations())\n",
    "    \n",
    "    print(\"Searching through {} permutations\".format(len(permutations)))\n",
    "\n",
    "    i = 0\n",
    "    for n in permutations:\n",
    "        grid_search_helper(n, models)\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"{} complete, time elapsed: {:.2f} minutes\".format(i, (time.time() - before) / 60))\n",
    "            \n",
    "        if i % 500 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "    models.sort(key=lambda n: n[0], reverse=True)\n",
    "\n",
    "    after = time.time()\n",
    "    print(\"Done, took {:.2f} minutes\".format((after - before) / 60))\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Wrapper function that allows garbage collector to run\n",
    "def grid_search_helper(permutation, models):\n",
    "    hyper_params = permutation_to_dictionary(*permutation)\n",
    "    model = Model(hyper_params)\n",
    "    model.to(device)\n",
    "    train_model(model)\n",
    "    accuracy = test_model(model, test_set)\n",
    "    models.append((accuracy, model.hyper_params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(models, filename):\n",
    "    params = [(accuracy, model) for (accuracy, model) in models]\n",
    "    params_json = json.dumps(params)\n",
    "    f = open(filename, \"w\")\n",
    "    f.write(params_json)\n",
    "    f.close()\n",
    "    \n",
    "def load_models(filename):\n",
    "    f = open(filename, \"r\")\n",
    "    params = json.loads(f.read())\n",
    "    f.close()\n",
    "    return [(accuracy, Model(hp)) for (accuracy, hp) in params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching through 1296 permutations\n",
      "100 complete, time elapsed: 10.53 minutes\n",
      "200 complete, time elapsed: 22.87 minutes\n",
      "300 complete, time elapsed: 33.82 minutes\n",
      "400 complete, time elapsed: 45.93 minutes\n",
      "500 complete, time elapsed: 57.15 minutes\n",
      "600 complete, time elapsed: 68.93 minutes\n",
      "700 complete, time elapsed: 81.25 minutes\n",
      "800 complete, time elapsed: 94.20 minutes\n",
      "900 complete, time elapsed: 107.75 minutes\n",
      "1000 complete, time elapsed: 120.51 minutes\n",
      "1100 complete, time elapsed: 134.29 minutes\n",
      "1200 complete, time elapsed: 146.82 minutes\n",
      "Done, took 160.52 minutes\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "models = grid_search()\n",
    "save_models(models, \"one_epoch_models.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.994 {'conv_layers': [{'in_channels': 1, 'out_channels': 8, 'kernel_size': 5}, {'in_channels': 8, 'out_channels': 16, 'kernel_size': 5}], 'regular_layers': [256, 128, 64, 10], 'activation_function': 'Swish', 'optimizer': 'SGD', 'criterion': 'CrossEntropy', 'learning_rate': 0.1, 'momentum': 0.5}\n",
      "0.99 {'conv_layers': [{'in_channels': 1, 'out_channels': 8, 'kernel_size': 5}, {'in_channels': 8, 'out_channels': 16, 'kernel_size': 5}], 'regular_layers': [256, 128, 10], 'activation_function': 'ReLU', 'optimizer': 'SGD', 'criterion': 'CrossEntropy', 'learning_rate': 0.1, 'momentum': 0.5}\n",
      "0.988 {'conv_layers': [{'in_channels': 1, 'out_channels': 8, 'kernel_size': 5}, {'in_channels': 8, 'out_channels': 16, 'kernel_size': 5}], 'regular_layers': [256, 128, 32, 10], 'activation_function': 'Swish', 'optimizer': 'Adam', 'criterion': 'CrossEntropy', 'learning_rate': 0.001, 'momentum': 0}\n",
      "0.987 {'conv_layers': [{'in_channels': 1, 'out_channels': 8, 'kernel_size': 5}, {'in_channels': 8, 'out_channels': 16, 'kernel_size': 5}], 'regular_layers': [256, 32, 10], 'activation_function': 'Swish', 'optimizer': 'Adam', 'criterion': 'CrossEntropy', 'learning_rate': 0.001, 'momentum': 0}\n",
      "0.986 {'conv_layers': [{'in_channels': 1, 'out_channels': 4, 'kernel_size': 5}, {'in_channels': 4, 'out_channels': 16, 'kernel_size': 5}], 'regular_layers': [256, 32, 10], 'activation_function': 'Swish', 'optimizer': 'Adam', 'criterion': 'CrossEntropy', 'learning_rate': 0.01, 'momentum': 0}\n"
     ]
    }
   ],
   "source": [
    "models = load_models(\"one_epoch_models.json\")\n",
    "for (acc, model) in models[:5]:\n",
    "    print(acc, model.hyper_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below plots a the accuracy of the networks prediction of a single handwritten number from the test_set.\n",
    "The generated graph displays all possible 10 numbers and shows which are the closest numbers that match the handwritten number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_classification(img, ps):\n",
    "    ''' Function for viewing an image and it's predicted classes.\n",
    "    '''\n",
    "    ps = ps.cpu().data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze(), cmap='gray', interpolation='none')\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    ax2.set_yticklabels(np.arange(10))\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to see how well our network generalize to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.953"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(model, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block fetches images and labels of the handwritten numbers. It fetches a single number from test_set and prints it out as an image to allow the user to know which handwritten number the network tried to predict.\n",
    "Together with the previously mentioned function, this allows to show the number that the network tries to guess together with the graph that displays the network's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight 6 1 5 5, but got 2-dimensional input of size [1, 784] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-d929e79093b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Turn off gradients to speed up this part\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mlogps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Output of the network are log-probabilities, need to take exponential for probabilities\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-724eed76f4cd>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[1;32m--> 342\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight 6 1 5 5, but got 2-dimensional input of size [1, 784] instead"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(test_set))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logps = model(img)\n",
    "\n",
    "# Output of the network are log-probabilities, need to take exponential for probabilities\n",
    "ps = torch.exp(logps)\n",
    "probab = list(ps.numpy()[0])\n",
    "print(\"Predicted Digit: \", probab.index(max(probab)))\n",
    "print(\"Original Label: \", labels[0].item())\n",
    "plot_image_classification(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight 6 1 5 5, but got 2-dimensional input of size [1, 784] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-2cd4d75f056e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m784\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[0mlogps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-724eed76f4cd>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[1;32m--> 342\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight 6 1 5 5, but got 2-dimensional input of size [1, 784] instead"
     ]
    }
   ],
   "source": [
    "correct_count, all_count = 0, 0\n",
    "predicted_labels = []\n",
    "actual_labels = []\n",
    "for images,labels in test_set:\n",
    "    for i in range(len(labels)):\n",
    "        img = images[i].view(1, 784)\n",
    "        with torch.no_grad():\n",
    "            logps = model(img)\n",
    "\n",
    "\n",
    "        ps = torch.exp(logps)\n",
    "        probab = list(ps.numpy()[0])\n",
    "        pred_label = probab.index(max(probab))\n",
    "        true_label = labels.numpy()[i]\n",
    "        \n",
    "        predicted_labels.append(pred_label)\n",
    "        actual_labels.append(true_label)\n",
    "        \n",
    "        if(true_label == pred_label):\n",
    "            correct_count += 1\n",
    "        all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(correct_count)\n",
    "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "           \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "#     print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (10, 10))\n",
    "    im = ax.imshow(cm,cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    \n",
    "    x_classes = classes \n",
    "    classes.reverse()\n",
    "    \n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),       \n",
    "           xticklabels=x_classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), ha=\"right\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    \n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            color=\"white\" if cm[i, j] > 500 else \"black\"\n",
    "            ax.text(j, i, format(cm[i, j]), ha=\"center\", va=\"center\", color=color)\n",
    "    \n",
    "    ax.set_xticks(np.arange(cm.shape[1]+1)-.5)\n",
    "    ax.set_yticks(np.arange(cm.shape[0]+1)-.5)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mkajs\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Attempting to set identical left == right == -0.5 results in singular transformations; automatically expanding.\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\mkajs\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Attempting to set identical bottom == top == -0.5 results in singular transformations; automatically expanding.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "label_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9,]\n",
    "\n",
    "plot_confusion_matrix(predicted_labels, actual_labels, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
