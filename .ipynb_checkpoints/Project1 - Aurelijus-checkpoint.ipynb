{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the libraries which are used for importing the dataset and creating the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from time import time\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create static variables that will be used later. These variables consists of values such as the rate at which the network learns from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f55d7a90f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "images, labels = next(iter(train_loader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logps = model(images) #log probabilities\n",
    "loss = criterion(logps, labels) #calculate the NLL loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward pass: \n",
      " None\n",
      "After backward pass: \n",
      " tensor([[ 0.0003,  0.0003,  0.0003,  ...,  0.0003,  0.0003,  0.0003],\n",
      "        [-0.0005, -0.0005, -0.0005,  ..., -0.0005, -0.0005, -0.0005],\n",
      "        [ 0.0009,  0.0009,  0.0009,  ...,  0.0009,  0.0009,  0.0009],\n",
      "        ...,\n",
      "        [ 0.0009,  0.0009,  0.0009,  ...,  0.0009,  0.0009,  0.0009],\n",
      "        [-0.0011, -0.0011, -0.0011,  ..., -0.0011, -0.0011, -0.0011],\n",
      "        [ 0.0002,  0.0002,  0.0002,  ...,  0.0002,  0.0002,  0.0002]])\n"
     ]
    }
   ],
   "source": [
    "print('Before backward pass: \\n', model[0].weight.grad)\n",
    "loss.backward()\n",
    "print('After backward pass: \\n', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.5083677045889755\n",
      "Epoch 1 - Training loss: 0.2031030416516448\n",
      "Epoch 2 - Training loss: 0.14794136295174518\n",
      "Epoch 3 - Training loss: 0.11679927920148189\n",
      "Epoch 4 - Training loss: 0.09485421864701106\n",
      "Epoch 5 - Training loss: 0.07890996041852655\n",
      "Epoch 6 - Training loss: 0.06685880800512538\n",
      "Epoch 7 - Training loss: 0.05688304985825743\n",
      "Epoch 8 - Training loss: 0.049619641295497195\n",
      "Epoch 9 - Training loss: 0.04345435459838946\n",
      "Epoch 10 - Training loss: 0.03819343294880823\n",
      "Epoch 11 - Training loss: 0.03292812477945706\n",
      "Epoch 12 - Training loss: 0.028960409296079796\n",
      "Epoch 13 - Training loss: 0.02551671211123228\n",
      "Epoch 14 - Training loss: 0.02231426579302608\n",
      "\n",
      "Training Time (in minutes) = 3.815260672569275\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
    "time0 = time()\n",
    "epochs = 15\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        #This is where the model learns by backpropagating\n",
    "        loss.backward()\n",
    "        \n",
    "        #And optimizes its weights here\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(train_loader)))\n",
    "print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_classify(img, ps):\n",
    "    ''' Function for viewing an image and it's predicted classes.\n",
    "    '''\n",
    "    ps = ps.cpu().data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    ax2.set_yticklabels(np.arange(10))\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Digit = 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(test_loader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logps = model(img)\n",
    "\n",
    "# Output of the network are log-probabilities, need to take exponential for probabilities\n",
    "ps = torch.exp(logps)\n",
    "probab = list(ps.numpy()[0])\n",
    "print(\"Predicted Digit =\", probab.index(max(probab)))\n",
    "view_classify(img.view(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
